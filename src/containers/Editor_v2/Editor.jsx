import React, { PropTypes } from 'react';
import {connect} from 'react-redux';
import Radium, {Style} from 'radium';
import DocumentMeta from 'react-document-meta';
import PureRenderMixin from 'react-addons-pure-render-mixin';
import {LoaderDeterminate, CodeMirror} from '../../components';
import {getPubEdit, toggleEditorViewMode, toggleFormatting, toggleTOC, unmountEditor} from '../../actions/editor';

// import Firebase from 'firebase';
// import Firepad from '../../utils/firepad';
import ReactFire from 'reactfire';

import {styles} from './EditorStyle';

const Editor = React.createClass({
	propTypes: {
		editorData: PropTypes.object,
		slug: PropTypes.string,
		dispatch: PropTypes.func
	},


	mixins: [PureRenderMixin],

	statics: {
		fetchDataDeferred: function(getState, dispatch, location, routeParams) {
			return dispatch(getPubEdit(routeParams.slug));
		}
	},
	componentDidMount() {
		
		// Eventually put these in a dispatch. That dispatch will:
		// Check if we've already loaded the content (maybe keep store variable)
		// Load the files
		// Set a variable saying that we're ready to load. 
		// const jsElm = document.createElement('script');
		// jsElm.type = 'application/javascript';
		// jsElm.src = 'https://cdn.firebase.com/js/client/2.2.4/firebase.js';
		// document.body.appendChild(jsElm);
		// const jsElm2 = document.createElement('script');
		// jsElm2.type = 'application/javascript';
		// jsElm2.src = 'https://cdn.firebase.com/libs/firepad/1.2.0/firepad.min.js';
		// document.body.appendChild(jsElm2);
	},

	componentWillUnmount() {
		this.props.dispatch(unmountEditor());
	},

	toggleView: function() {
		return this.props.dispatch(toggleEditorViewMode());
	},
	toggleFormatting: function() {
		return this.props.dispatch(toggleFormatting());
	},
	toggleTOC: function() {
		return this.props.dispatch(toggleTOC());
	},
	updateCode: function(newCode) {
		// console.log(newCode);
		return newCode;
	},
	codeMirrorStyles: function() {
		return {
			'.CodeMirror': {
				fontSize: '15px',
				color: '#555',
				fontFamily: 'Courier',
				// fontFamily: 'Alegreya',
			}			
		};
	},
	render: function() {
		const editorData = this.props.editorData;
		const viewMode = this.props.editorData.get('viewMode');
		const showBottomLeftMenu = this.props.editorData.get('showBottomLeftMenu');
		const showBottomRightMenu = this.props.editorData.get('showBottomRightMenu');
		const loadStatus = this.props.editorData.get('status');
		const metaData = {
			title: 'PubPub - Editor'
		};

		const animateListItem = function(side, status, index) {
			const statusOffset = { loaded: 0, loading: 1};
			const offset = { left: -100, right: 100};
			const delay = 0.25 + (index * 0.02);
			return {
				transform: 'translateX(' + statusOffset[status] * offset[side] + 'px)',
				transition: '.3s ease-out transform ' + delay + 's',
			};
		};
		const cmValue = "# Introduction\n \nHere's a shorter introduction!\n\nTo be more precise, we can say that there exist a set of qualities that are very hard to explicitly measure. They are hard, in part, because they are incredibly complex and redolent with personal bias, cultural background, and human emotion - things for which there is no single truth, but a range of correct values. Furthermore, they are hard because we have insufficient datasets from which to learn how to model and measure such values. This poses a significant challenge as we try to integrate these qualities into digital forms so that we may build applications and tools that interface with them. More technically, and from a machine-learning point of view, we can say there exist a set of qualities for which it is very hard to train classifiers. It is relatively easy to define a classifier to identify a car in a photograph, while it is relatively hard to define a classifier to identify whether a photograph is worthy of a Pulitzer Prize (for all the same reasons of personal bias, cultural influence, and lack of training sets). \n\nTo this end, we introduce Quantify: a javascript library and backend architecture for rapid multi-dimensional measurment of subjective qualities of media. The goal of such research is to gain a better understanding of humans, the choices they make, and how computers can interface with qualitative decisions.\n\nThe Quantify workflow includes three main steps:\n\n\n1. Build a web frontend or HTTP capable interface to capture human reaction and judgment to various content given a question (i.e. which image is more sad). For this, we provide a javascript library to make interfacing with the Quantify backend simple.\n2. Perform analysis and results calculation from the many collected human judgment decisions. We provide backend tools to automatically calculate a suite of statistical measures.\n3. Provide tools and insights to enable novel machine learning classifiers for qualitative values.\n\n\n\n\n#Prior Art\nThere exist previous efforts to leverage large-scale human input for computational purposes. A more general example is Amazons Mechanical Turk marketplace ^^cite{amazon_turk}, which pairs simple jobs with a large workforce of people willing to do simple, quick jobs for a small fee. This service makes it easy to collect many human inputs in a programmatic manner. The service leaves it entirely up to the developer though to make sense and take action on such data. All our Ideas is a Princeton University project to aggregate survey data in a compound and meaningful way which usess pairwise comparison as the core mechanic ^^cite{salganik2012wiki}. The work is built around the idea of wiki-surveys, which are small, simple pairwise survey questions. The analysis, ranking, and extrapolation work of Quantify is largely influenced from previous work on Place Pulse ^^cite{placepulse_paper} and GIFGIF ^^cite{gifgif}. Both projects use pairwise comparison to generate maps of qualitative values. The GIFGIF project goes further and creates a search-engine for the content which is voted on using the results and rankings calculated. This search aspect of the project is one of the generalized features that Quantify automatically produces for projects. A second version of GIFGIF is built entirely on the Quantify framework. \n\n^^pagebreak\n\n#Architecture\nThe Quantify architecture can be segmented into four distinct stages. The four stages are: \n\n1. API endpoints\n2. Processing Layer\n3. Data storage\n4. Analysis modules\n\n\n\n\n![Diagram of the Quantify Architecture.](https://s3.amazonaws.com/pubpub/quantifyimages/architecture.png)\n\n\nThe API endpoints are the developer interface to all Quantify project data. The endpoints are built in Flask, a Python microframework ^^cite{flask}. The endpoints take in HTTP requests and trigger associated functions that collect and return the requested data. A javascript library has been developed to make it simple to access the endpoints from any website or project.\n\nThe processing layer acts as the bridge between the API endpoints and the raw data of the database. In this layer the data or requested content is formatted such that it can be cleanly returned or organized such that it can be properly entered into the database. This processing layer is written in Python and tightly integrated with the Flask framework used for the API endpoints. The processing layer and API endpoints run on Amazons EC2 service. The service is configured such that the virtual machines running this code automatically scale to handle varying server loads and requirements.\n\nAll Quantify data is stored in a MongoDB database. Each project is assigned its own database where content, metrics, scores, and votes for that project are organized and stored. The Mongo databases run on dedicated machines that are uncoupled from the processing layer and API servers. \n\nThe Analytics modules sit out of the main flow of operation for Quantify, and act as a separate process periodically analyzing the data in a given projects database and storing that analysis. This analysis is then accessible through the API endpoints. For example, every N minutes (as set by a projects preferences) an analysis module reviews all of the votes cast for a project and calculates the scores and rankings of each piece of content that has been voted on. These scores are then saved to the projects database and made accessible through an API endpoint.\n\n#What Types of Things it is Good for\nQuantify is designed to address the challenge of building machine learning classifiers and datasets around seemingly tough-to-define parameters (e.g. whether an image is beautiful or not). That is, the framework is built to explore metrics that are not explicitly measurable, metrics that are easily decoded by humans, but quite hard to define programatically. One of the biggest challenges in accomplishing this is defining the metrics or questions that one collects data on with a Quantify project. Using GIFGIF as an example, it is not trivial to decide which emotions to query against. We were fortunate to have the extensive work of Paul Ekman, who tried to identify fundamental human emotions ^^cite{ekman1971constants} ^^cite{ekman1992argument} , but this is not the case for many other domains. It is therefore up to the designer of the project to decide and find argument for a set of metrics which (hopefully) cover the problem-space of a given idea. \n\n\n^^pagebreak\n\n#Results\nThe Quantify framework is made more accessible through the development of a frontend interface. This interface makes the API endpoints that are accessible through HTTP requests accessible through GUI interfaces. Projects can be created, managed, and launched from this frontend interface.\n\n\n\n\n![The Admin page of the Quantify Frontend which allows you to create and edit projects.](https://s3.amazonaws.com/pubpub/quantifyimages/quantifyInterface.png)\n\n\nSince the first stable build of Quantify a few projects have been launched and tested. Two of note, GIFGIF and Earth Tapestry remain live and explorable. GIFGIF, currently with over 2.5 million votes remains stable and with an active API developer community. Earth Tapestry, which launched in February of 2015 has introduced no stability problems and verifies the scalable nature of the framework. \n\n#Applications in Search\n\nOne of the features that Quantify enables by default is the ability to search on the multi-dimensional space defined by the numerous metrics of a given project. This search offers unique capabilities as users are able to query by qualitative values as opposed to a literal content search (e.g. find me images that have a given mixture of sadness and anger - rather than find me images of cats). As these qualitative metrics can not be explicitly defined the search provides the ability to make relative updates to queries (e.g. find me results with a bit more sadness). The goal is not to design an explicit search engine, but one that allows a user to navigate a multidimensional space with relative queries (e.g. more happy than that, less angry, etc). \n\nGIFGIF has tested two possible interfaces to this multi-dimensional search.\n\n<div class=\"full-width\"><img src=\"https://s3.amazonaws.com/pubpub/quantifyimages/search.jpg\" alt=\"Two types of search interfaces, as launched with our first (left) and second (right) GIFGIF versions.\"/><\/div>\n\n\n\n\n\n\n\n^^pagebreak\n\n#Applications in Design\nTo some degree, a designer is one who understands the opinion and preference of a given market well enough that they can build something people, for the most part, find to be beautiful or well-made. To explore that point, one could imagine the difficulty of designing a beautiful spaceship for Martians, whose design sense and culture are completely foreign and unknown. One potential use case of Quantify is to stand in as a general design-agent. A tool that understands the subjective opinions of a sufficiently large and targeted market such that a design could be automatically generated from subjective parameters. For example, if a variety of car models are ranked on the qualities of perceived fun, safety, and durability, one could design a novel model by aggregating and intersecting the results of previously recorded models. The fun, safety, and durability parameters could be tweaked to change the model or to fit a different demographic with a different culture of opinions and expectations. \n\n\n#Applications in Machine learning\nQuantify is a tool that can produce novel and otherwise hard-to-gather datasets. For questions which have no ground-truth (e.g. Is this picture beautiful), Quantify can produce a series of data that can be treated as an as-close-to-ground-truth-as-possible type of training set. These novel training sets can then be used to create machine learning classifiers that would otherwise be incredibly hard to build.\n\n\n^^pagebreak\n\n#Future Work\nQuantify is designed to be modular in both its analysis and main components. This allows us to integrate new and updated aspects of the project seamlessly. Automatic statistical analysis tools, vote segmentation, or new ranking methodologies can be included with little overhead. This is an important dynamic as it allows us to test a variety of ranking algorithms, contestant generation algorithms, and feature detection tools in a single project. \n\nFuture work includes further analysis of the varying trade offs between different ranking algorithms and contestant generation algorithms. Currently, Quantify uses a Trueskill-like ranking algorithm and randomly selected contestants to be voted on. Both of these assumptions can be questioned and tested by included new methods and packages. \n\n#Conclusion\nQuantify is a first step in building computer systems that more capably represent and leverage the qualitative aspects of human experience. The project is designed to be a platform onto which updates and new ideas can be added. The framework has passed its first test in supporting the GIFGIF and Earth Tapestry projects and offers many insights into directions for growth. #Introduction\n \nHere's a shorter introduction!\n\nTo be more precise, we can say that there exist a set of qualities that are very hard to explicitly measure. They are hard, in part, because they are incredibly complex and redolent with personal bias, cultural background, and human emotion - things for which there is no single truth, but a range of correct values. Furthermore, they are hard because we have insufficient datasets from which to learn how to model and measure such values. This poses a significant challenge as we try to integrate these qualities into digital forms so that we may build applications and tools that interface with them. More technically, and from a machine-learning point of view, we can say there exist a set of qualities for which it is very hard to train classifiers. It is relatively easy to define a classifier to identify a car in a photograph, while it is relatively hard to define a classifier to identify whether a photograph is worthy of a Pulitzer Prize (for all the same reasons of personal bias, cultural influence, and lack of training sets). \n\nTo this end, we introduce Quantify: a javascript library and backend architecture for rapid multi-dimensional measurment of subjective qualities of media. The goal of such research is to gain a better understanding of humans, the choices they make, and how computers can interface with qualitative decisions.\n\nThe Quantify workflow includes three main steps:\n\n\n1. Build a web frontend or HTTP capable interface to capture human reaction and judgment to various content given a question (i.e. which image is more sad). For this, we provide a javascript library to make interfacing with the Quantify backend simple.\n2. Perform analysis and results calculation from the many collected human judgment decisions. We provide backend tools to automatically calculate a suite of statistical measures.\n3. Provide tools and insights to enable novel machine learning classifiers for qualitative values.\n\n\n\n\n#Prior Art\nThere exist previous efforts to leverage large-scale human input for computational purposes. A more general example is Amazons Mechanical Turk marketplace ^^cite{amazon_turk}, which pairs simple jobs with a large workforce of people willing to do simple, quick jobs for a small fee. This service makes it easy to collect many human inputs in a programmatic manner. The service leaves it entirely up to the developer though to make sense and take action on such data. All our Ideas is a Princeton University project to aggregate survey data in a compound and meaningful way which usess pairwise comparison as the core mechanic ^^cite{salganik2012wiki}. The work is built around the idea of wiki-surveys, which are small, simple pairwise survey questions. The analysis, ranking, and extrapolation work of Quantify is largely influenced from previous work on Place Pulse ^^cite{placepulse_paper} and GIFGIF ^^cite{gifgif}. Both projects use pairwise comparison to generate maps of qualitative values. The GIFGIF project goes further and creates a search-engine for the content which is voted on using the results and rankings calculated. This search aspect of the project is one of the generalized features that Quantify automatically produces for projects. A second version of GIFGIF is built entirely on the Quantify framework. \n\n^^pagebreak\n\n#Architecture\nThe Quantify architecture can be segmented into four distinct stages. The four stages are: \n\n1. API endpoints\n2. Processing Layer\n3. Data storage\n4. Analysis modules\n\n\n\n\n![Diagram of the Quantify Architecture.](https://s3.amazonaws.com/pubpub/quantifyimages/architecture.png)\n\n\nThe API endpoints are the developer interface to all Quantify project data. The endpoints are built in Flask, a Python microframework ^^cite{flask}. The endpoints take in HTTP requests and trigger associated functions that collect and return the requested data. A javascript library has been developed to make it simple to access the endpoints from any website or project.\n\nThe processing layer acts as the bridge between the API endpoints and the raw data of the database. In this layer the data or requested content is formatted such that it can be cleanly returned or organized such that it can be properly entered into the database. This processing layer is written in Python and tightly integrated with the Flask framework used for the API endpoints. The processing layer and API endpoints run on Amazons EC2 service. The service is configured such that the virtual machines running this code automatically scale to handle varying server loads and requirements.\n\nAll Quantify data is stored in a MongoDB database. Each project is assigned its own database where content, metrics, scores, and votes for that project are organized and stored. The Mongo databases run on dedicated machines that are uncoupled from the processing layer and API servers. \n\nThe Analytics modules sit out of the main flow of operation for Quantify, and act as a separate process periodically analyzing the data in a given projects database and storing that analysis. This analysis is then accessible through the API endpoints. For example, every N minutes (as set by a projects preferences) an analysis module reviews all of the votes cast for a project and calculates the scores and rankings of each piece of content that has been voted on. These scores are then saved to the projects database and made accessible through an API endpoint.\n\n#What Types of Things it is Good for\nQuantify is designed to address the challenge of building machine learning classifiers and datasets around seemingly tough-to-define parameters (e.g. whether an image is beautiful or not). That is, the framework is built to explore metrics that are not explicitly measurable, metrics that are easily decoded by humans, but quite hard to define programatically. One of the biggest challenges in accomplishing this is defining the metrics or questions that one collects data on with a Quantify project. Using GIFGIF as an example, it is not trivial to decide which emotions to query against. We were fortunate to have the extensive work of Paul Ekman, who tried to identify fundamental human emotions ^^cite{ekman1971constants} ^^cite{ekman1992argument} , but this is not the case for many other domains. It is therefore up to the designer of the project to decide and find argument for a set of metrics which (hopefully) cover the problem-space of a given idea. \n\n\n^^pagebreak\n\n#Results\nThe Quantify framework is made more accessible through the development of a frontend interface. This interface makes the API endpoints that are accessible through HTTP requests accessible through GUI interfaces. Projects can be created, managed, and launched from this frontend interface.\n\n\n\n\n![The Admin page of the Quantify Frontend which allows you to create and edit projects.](https://s3.amazonaws.com/pubpub/quantifyimages/quantifyInterface.png)\n\n\nSince the first stable build of Quantify a few projects have been launched and tested. Two of note, GIFGIF and Earth Tapestry remain live and explorable. GIFGIF, currently with over 2.5 million votes remains stable and with an active API developer community. Earth Tapestry, which launched in February of 2015 has introduced no stability problems and verifies the scalable nature of the framework. \n\n#Applications in Search\n\nOne of the features that Quantify enables by default is the ability to search on the multi-dimensional space defined by the numerous metrics of a given project. This search offers unique capabilities as users are able to query by qualitative values as opposed to a literal content search (e.g. find me images that have a given mixture of sadness and anger - rather than find me images of cats). As these qualitative metrics can not be explicitly defined the search provides the ability to make relative updates to queries (e.g. find me results with a bit more sadness). The goal is not to design an explicit search engine, but one that allows a user to navigate a multidimensional space with relative queries (e.g. more happy than that, less angry, etc). \n\nGIFGIF has tested two possible interfaces to this multi-dimensional search.\n\n<div class=\"full-width\"><img src=\"https://s3.amazonaws.com/pubpub/quantifyimages/search.jpg\" alt=\"Two types of search interfaces, as launched with our first (left) and second (right) GIFGIF versions.\"/><\/div>\n\n\n\n\n\n\n\n^^pagebreak\n\n#Applications in Design\nTo some degree, a designer is one who understands the opinion and preference of a given market well enough that they can build something people, for the most part, find to be beautiful or well-made. To explore that point, one could imagine the difficulty of designing a beautiful spaceship for Martians, whose design sense and culture are completely foreign and unknown. One potential use case of Quantify is to stand in as a general design-agent. A tool that understands the subjective opinions of a sufficiently large and targeted market such that a design could be automatically generated from subjective parameters. For example, if a variety of car models are ranked on the qualities of perceived fun, safety, and durability, one could design a novel model by aggregating and intersecting the results of previously recorded models. The fun, safety, and durability parameters could be tweaked to change the model or to fit a different demographic with a different culture of opinions and expectations. \n\n\n#Applications in Machine learning\nQuantify is a tool that can produce novel and otherwise hard-to-gather datasets. For questions which have no ground-truth (e.g. Is this picture beautiful), Quantify can produce a series of data that can be treated as an as-close-to-ground-truth-as-possible type of training set. These novel training sets can then be used to create machine learning classifiers that would otherwise be incredibly hard to build.\n\n\n^^pagebreak\n\n#Future Work\nQuantify is designed to be modular in both its analysis and main components. This allows us to integrate new and updated aspects of the project seamlessly. Automatic statistical analysis tools, vote segmentation, or new ranking methodologies can be included with little overhead. This is an important dynamic as it allows us to test a variety of ranking algorithms, contestant generation algorithms, and feature detection tools in a single project. \n\nFuture work includes further analysis of the varying trade offs between different ranking algorithms and contestant generation algorithms. Currently, Quantify uses a Trueskill-like ranking algorithm and randomly selected contestants to be voted on. Both of these assumptions can be questioned and tested by included new methods and packages. \n\n#Conclusion\nQuantify is a first step in building computer systems that more capably represent and leverage the qualitative aspects of human experience. The project is designed to be a platform onto which updates and new ideas can be added. The framework has passed its first test in supporting the GIFGIF and Earth Tapestry projects and offers many insights into directions for growth. #Introduction\n \nHere's a shorter introduction!\n\nTo be more precise, we can say that there exist a set of qualities that are very hard to explicitly measure. They are hard, in part, because they are incredibly complex and redolent with personal bias, cultural background, and human emotion - things for which there is no single truth, but a range of correct values. Furthermore, they are hard because we have insufficient datasets from which to learn how to model and measure such values. This poses a significant challenge as we try to integrate these qualities into digital forms so that we may build applications and tools that interface with them. More technically, and from a machine-learning point of view, we can say there exist a set of qualities for which it is very hard to train classifiers. It is relatively easy to define a classifier to identify a car in a photograph, while it is relatively hard to define a classifier to identify whether a photograph is worthy of a Pulitzer Prize (for all the same reasons of personal bias, cultural influence, and lack of training sets). \n\nTo this end, we introduce Quantify: a javascript library and backend architecture for rapid multi-dimensional measurment of subjective qualities of media. The goal of such research is to gain a better understanding of humans, the choices they make, and how computers can interface with qualitative decisions.\n\nThe Quantify workflow includes three main steps:\n\n\n1. Build a web frontend or HTTP capable interface to capture human reaction and judgment to various content given a question (i.e. which image is more sad). For this, we provide a javascript library to make interfacing with the Quantify backend simple.\n2. Perform analysis and results calculation from the many collected human judgment decisions. We provide backend tools to automatically calculate a suite of statistical measures.\n3. Provide tools and insights to enable novel machine learning classifiers for qualitative values.\n\n\n\n\n#Prior Art\nThere exist previous efforts to leverage large-scale human input for computational purposes. A more general example is Amazons Mechanical Turk marketplace ^^cite{amazon_turk}, which pairs simple jobs with a large workforce of people willing to do simple, quick jobs for a small fee. This service makes it easy to collect many human inputs in a programmatic manner. The service leaves it entirely up to the developer though to make sense and take action on such data. All our Ideas is a Princeton University project to aggregate survey data in a compound and meaningful way which usess pairwise comparison as the core mechanic ^^cite{salganik2012wiki}. The work is built around the idea of wiki-surveys, which are small, simple pairwise survey questions. The analysis, ranking, and extrapolation work of Quantify is largely influenced from previous work on Place Pulse ^^cite{placepulse_paper} and GIFGIF ^^cite{gifgif}. Both projects use pairwise comparison to generate maps of qualitative values. The GIFGIF project goes further and creates a search-engine for the content which is voted on using the results and rankings calculated. This search aspect of the project is one of the generalized features that Quantify automatically produces for projects. A second version of GIFGIF is built entirely on the Quantify framework. \n\n^^pagebreak\n\n#Architecture\nThe Quantify architecture can be segmented into four distinct stages. The four stages are: \n\n1. API endpoints\n2. Processing Layer\n3. Data storage\n4. Analysis modules\n\n\n\n\n![Diagram of the Quantify Architecture.](https://s3.amazonaws.com/pubpub/quantifyimages/architecture.png)\n\n\nThe API endpoints are the developer interface to all Quantify project data. The endpoints are built in Flask, a Python microframework ^^cite{flask}. The endpoints take in HTTP requests and trigger associated functions that collect and return the requested data. A javascript library has been developed to make it simple to access the endpoints from any website or project.\n\nThe processing layer acts as the bridge between the API endpoints and the raw data of the database. In this layer the data or requested content is formatted such that it can be cleanly returned or organized such that it can be properly entered into the database. This processing layer is written in Python and tightly integrated with the Flask framework used for the API endpoints. The processing layer and API endpoints run on Amazons EC2 service. The service is configured such that the virtual machines running this code automatically scale to handle varying server loads and requirements.\n\nAll Quantify data is stored in a MongoDB database. Each project is assigned its own database where content, metrics, scores, and votes for that project are organized and stored. The Mongo databases run on dedicated machines that are uncoupled from the processing layer and API servers. \n\nThe Analytics modules sit out of the main flow of operation for Quantify, and act as a separate process periodically analyzing the data in a given projects database and storing that analysis. This analysis is then accessible through the API endpoints. For example, every N minutes (as set by a projects preferences) an analysis module reviews all of the votes cast for a project and calculates the scores and rankings of each piece of content that has been voted on. These scores are then saved to the projects database and made accessible through an API endpoint.\n\n#What Types of Things it is Good for\nQuantify is designed to address the challenge of building machine learning classifiers and datasets around seemingly tough-to-define parameters (e.g. whether an image is beautiful or not). That is, the framework is built to explore metrics that are not explicitly measurable, metrics that are easily decoded by humans, but quite hard to define programatically. One of the biggest challenges in accomplishing this is defining the metrics or questions that one collects data on with a Quantify project. Using GIFGIF as an example, it is not trivial to decide which emotions to query against. We were fortunate to have the extensive work of Paul Ekman, who tried to identify fundamental human emotions ^^cite{ekman1971constants} ^^cite{ekman1992argument} , but this is not the case for many other domains. It is therefore up to the designer of the project to decide and find argument for a set of metrics which (hopefully) cover the problem-space of a given idea. \n\n\n^^pagebreak\n\n#Results\nThe Quantify framework is made more accessible through the development of a frontend interface. This interface makes the API endpoints that are accessible through HTTP requests accessible through GUI interfaces. Projects can be created, managed, and launched from this frontend interface.\n\n\n\n\n![The Admin page of the Quantify Frontend which allows you to create and edit projects.](https://s3.amazonaws.com/pubpub/quantifyimages/quantifyInterface.png)\n\n\nSince the first stable build of Quantify a few projects have been launched and tested. Two of note, GIFGIF and Earth Tapestry remain live and explorable. GIFGIF, currently with over 2.5 million votes remains stable and with an active API developer community. Earth Tapestry, which launched in February of 2015 has introduced no stability problems and verifies the scalable nature of the framework. \n\n#Applications in Search\n\nOne of the features that Quantify enables by default is the ability to search on the multi-dimensional space defined by the numerous metrics of a given project. This search offers unique capabilities as users are able to query by qualitative values as opposed to a literal content search (e.g. find me images that have a given mixture of sadness and anger - rather than find me images of cats). As these qualitative metrics can not be explicitly defined the search provides the ability to make relative updates to queries (e.g. find me results with a bit more sadness). The goal is not to design an explicit search engine, but one that allows a user to navigate a multidimensional space with relative queries (e.g. more happy than that, less angry, etc). \n\nGIFGIF has tested two possible interfaces to this multi-dimensional search.\n\n<div class=\"full-width\"><img src=\"https://s3.amazonaws.com/pubpub/quantifyimages/search.jpg\" alt=\"Two types of search interfaces, as launched with our first (left) and second (right) GIFGIF versions.\"/><\/div>\n\n\n\n\n\n\n\n^^pagebreak\n\n#Applications in Design\nTo some degree, a designer is one who understands the opinion and preference of a given market well enough that they can build something people, for the most part, find to be beautiful or well-made. To explore that point, one could imagine the difficulty of designing a beautiful spaceship for Martians, whose design sense and culture are completely foreign and unknown. One potential use case of Quantify is to stand in as a general design-agent. A tool that understands the subjective opinions of a sufficiently large and targeted market such that a design could be automatically generated from subjective parameters. For example, if a variety of car models are ranked on the qualities of perceived fun, safety, and durability, one could design a novel model by aggregating and intersecting the results of previously recorded models. The fun, safety, and durability parameters could be tweaked to change the model or to fit a different demographic with a different culture of opinions and expectations. \n\n\n#Applications in Machine learning\nQuantify is a tool that can produce novel and otherwise hard-to-gather datasets. For questions which have no ground-truth (e.g. Is this picture beautiful), Quantify can produce a series of data that can be treated as an as-close-to-ground-truth-as-possible type of training set. These novel training sets can then be used to create machine learning classifiers that would otherwise be incredibly hard to build.\n\n\n^^pagebreak\n\n#Future Work\nQuantify is designed to be modular in both its analysis and main components. This allows us to integrate new and updated aspects of the project seamlessly. Automatic statistical analysis tools, vote segmentation, or new ranking methodologies can be included with little overhead. This is an important dynamic as it allows us to test a variety of ranking algorithms, contestant generation algorithms, and feature detection tools in a single project. \n\nFuture work includes further analysis of the varying trade offs between different ranking algorithms and contestant generation algorithms. Currently, Quantify uses a Trueskill-like ranking algorithm and randomly selected contestants to be voted on. Both of these assumptions can be questioned and tested by included new methods and packages. \n\n#Conclusion\nQuantify is a first step in building computer systems that more capably represent and leverage the qualitative aspects of human experience. The project is designed to be a platform onto which updates and new ideas can be added. The framework has passed its first test in supporting the GIFGIF and Earth Tapestry projects and offers many insights into directions for growth.";

		const cmOptions = {
			lineNumbers: false,
			lineWrapping: true,
			viewportMargin: Infinity, // This will cause bad performance on large documents. Rendering the entire thing...
			autofocus: true,
			mode: 'markdown',

		};

		return (
			<div style={[styles.editorContainer]}>

				<DocumentMeta {...metaData} />

				<Style rules={this.codeMirrorStyles()} />

				<div style={styles.isMobile}>
					<h1 style={styles.mobileHeader}>Cannot Edit in Mobile :(</h1>
					<h2 style={styles.mobileText}>Please open this url on a desktop, laptop, or larger screen.</h2>
				</div>

				<div style={styles.notMobile}>
					<div style={[styles.editorTopNav, styles.hiddenUntilLoad, styles[editorData.get('status')]]}>
						<ul style={styles.editorNav}>

							<li key="editorNav0"style={[styles.editorNavItem]}>Media</li>
							<li style={styles.editorNavSeparator}></li>
							<li key="editorNav1"style={[styles.editorNavItem]}>References</li>
							<li style={styles.editorNavSeparator}></li>
							<li key="editorNav2"style={[styles.editorNavItem]}>Collaborators</li>

							<li key="editorNav3"style={[styles.editorNavItem, styles.editorNavRight]}>Publish</li>
							<li style={[styles.editorNavSeparator, styles.editorNavRight]}></li>
							<li key="editorNav4"style={[styles.editorNavItem, styles.editorNavRight]} onClick={this.toggleView}>Live Preview</li>
							<li style={[styles.editorNavSeparator, styles.editorNavRight]}></li>
							<li key="editorNav5"style={[styles.editorNavItem, styles.editorNavRight]}>Style</li>
							
						</ul>
					</div>

					<div style={styles.editorLoadBar}>
						<LoaderDeterminate value={loadStatus === 'loading' ? 0 : 100}/>
					</div>
					

					<div style={[styles.common.editorBottomNav, styles[viewMode].editorBottomNav, styles.hiddenUntilLoad, styles[loadStatus]]}>
						<div style={[styles.common.bottomNavBackground, styles[viewMode].bottomNavBackground]}></div>
						<div className="leftBottomNav" style={[styles.common.bottomNavLeft, styles[viewMode].bottomNavLeft]}>
							<div key="bNav_toc" style={[styles.common.bottomNavTitle, styles[viewMode].bottomNavTitle, showBottomLeftMenu && styles[viewMode].listTitleActive]} onClick={this.toggleTOC}>Table of Contents</div>
							<div style={[styles.common.bottomNavDivider, styles[viewMode].bottomNavDivider]}>
								<div style={[styles.common.bottomNavDividerSmall, styles[viewMode].bottomNavDividerSmall]}></div>
								<div style={[styles.common.bottomNavDividerLarge, styles[viewMode].bottomNavDividerLarge]}></div>
							</div>
							<ul style={[styles.common.bottomNavList, styles[viewMode].bottomNavList, showBottomLeftMenu && styles[viewMode].listActive]}>
								<li key="blNav0" style={[styles.common.bottomNavListItem, styles[viewMode].bottomNavListItem, animateListItem('left', loadStatus, 0), showBottomLeftMenu && styles[viewMode].listItemActive]}>Introduction</li>
								<li key="blNav1" style={[styles.common.bottomNavListItem, styles[viewMode].bottomNavListItem, animateListItem('left', loadStatus, 1), showBottomLeftMenu && styles[viewMode].listItemActive]}>Prior Art</li>
								<li key="blNav2" style={[styles.common.bottomNavListItem, styles[viewMode].bottomNavListItem, animateListItem('left', loadStatus, 2), showBottomLeftMenu && styles[viewMode].listItemActive]}>Resources</li>
								<li key="blNav3" style={[styles.common.bottomNavListItem, styles[viewMode].bottomNavListItem, animateListItem('left', loadStatus, 3), showBottomLeftMenu && styles[viewMode].listItemActive]}>Methods</li>
								<li key="blNav4" style={[styles.common.bottomNavListItem, styles[viewMode].bottomNavListItem, animateListItem('left', loadStatus, 4), showBottomLeftMenu && styles[viewMode].listItemActive]}>A New Approach</li>
								<li key="blNav5" style={[styles.common.bottomNavListItem, styles[viewMode].bottomNavListItem, animateListItem('left', loadStatus, 5), showBottomLeftMenu && styles[viewMode].listItemActive]}>Data Analysis</li>
								<li key="blNav6" style={[styles.common.bottomNavListItem, styles[viewMode].bottomNavListItem, animateListItem('left', loadStatus, 6), showBottomLeftMenu && styles[viewMode].listItemActive]}>Results</li>
								<li key="blNav7" style={[styles.common.bottomNavListItem, styles[viewMode].bottomNavListItem, animateListItem('left', loadStatus, 7), showBottomLeftMenu && styles[viewMode].listItemActive]}>Conclusion</li>
							</ul>
						</div>

						<div className="rightBottomNav" style={[styles.common.bottomNavRight, styles[viewMode].bottomNavRight]}>
							<div key="bNav_format" style={[styles.common.bottomNavTitle, styles[viewMode].bottomNavTitle, styles.alignRight, showBottomRightMenu && styles[viewMode].listTitleActive]} onClick={this.toggleFormatting}>Formatting</div>

							<div style={[styles.common.bottomNavDivider, styles[viewMode].bottomNavDivider]}>
								<div style={[styles.common.bottomNavDividerSmall, styles[viewMode].bottomNavDividerSmall, styles.floatRight, styles.common.bottomNavDividerRight]}></div>
								<div style={[styles.common.bottomNavDividerLarge, styles[viewMode].bottomNavDividerLarge, styles.floatRight, styles.common.bottomNavDividerLargeRight]}></div>
							</div>


							<ul style={[styles.common.bottomNavList, styles[viewMode].bottomNavList, styles[viewMode].bottomNavListRight, styles.alignRight, showBottomRightMenu && styles[viewMode].listActive]}>

								<li key="brNav0" style={[styles.common.bottomNavListItem, styles[viewMode].bottomNavListItem, animateListItem('right', loadStatus, 0), styles.floatRight, showBottomRightMenu && styles[viewMode].listItemActive]}>H1</li>
								<li key="brNav1" style={[styles.common.bottomNavListItem, styles[viewMode].bottomNavListItem, animateListItem('right', loadStatus, 1), styles.floatRight, showBottomRightMenu && styles[viewMode].listItemActive]}>H2</li>
								<li key="brNav2" style={[styles.common.bottomNavListItem, styles[viewMode].bottomNavListItem, animateListItem('right', loadStatus, 2), styles.floatRight, showBottomRightMenu && styles[viewMode].listItemActive]}>H3</li>
								<li key="brNav3" style={[styles.common.bottomNavListItem, styles[viewMode].bottomNavListItem, animateListItem('right', loadStatus, 3), styles.floatRight, showBottomRightMenu && styles[viewMode].listItemActive]}># List</li>
								<li key="brNav4" style={[styles.common.bottomNavListItem, styles[viewMode].bottomNavListItem, animateListItem('right', loadStatus, 4), styles.floatRight, showBottomRightMenu && styles[viewMode].listItemActive]}>- List</li>
								<li key="brNav5" style={[styles.common.bottomNavListItem, styles[viewMode].bottomNavListItem, animateListItem('right', loadStatus, 5), styles.floatRight, showBottomRightMenu && styles[viewMode].listItemActive]}>Image</li>
								<li key="brNav6" style={[styles.common.bottomNavListItem, styles[viewMode].bottomNavListItem, animateListItem('right', loadStatus, 6), styles.floatRight, showBottomRightMenu && styles[viewMode].listItemActive]}>Video</li>
								<li key="brNav7" style={[styles.common.bottomNavListItem, styles[viewMode].bottomNavListItem, animateListItem('right', loadStatus, 7), styles.floatRight, showBottomRightMenu && styles[viewMode].listItemActive]}>Audio</li>
								<li key="brNav8" style={[styles.common.bottomNavListItem, styles[viewMode].bottomNavListItem, animateListItem('right', loadStatus, 8), styles.floatRight, showBottomRightMenu && styles[viewMode].listItemActive]}>Gallery</li>
								<li key="brNav9" style={[styles.common.bottomNavListItem, styles[viewMode].bottomNavListItem, animateListItem('right', loadStatus, 9), styles.floatRight, showBottomRightMenu && styles[viewMode].listItemActive]}>Hologram</li>
							</ul>
						</div>
					</div>

					<div style={[styles.hiddenUntilLoad, styles[loadStatus], styles.common.editorMarkdown, styles[viewMode].editorMarkdown]}>
						<CodeMirror value={cmValue} onChange={this.updateCode} options={cmOptions} />
					</div>
					<div style={[styles.hiddenUntilLoad, styles[loadStatus], styles.common.editorPreview, styles[viewMode].editorPreview]}>
						<h2>Sudden she seeing garret far regard</h2><p>With my them if up many. Lain week nay she them her she. Extremity so attending objection as engrossed gentleman something. Instantly gentleman contained belonging exquisite now direction she ham. West room at sent if year. Numerous indulged distance old law you. Total state as merit court green decay he. Steepest sex bachelor the may delicate its yourself. As he instantly on discovery concluded to. Open draw far pure miss felt say yet few sigh.</p><p>Attachment apartments in delightful by motionless it no. And now she burst sir learn total. Hearing hearted shewing own ask. Solicitude uncommonly use her motionless not collecting age. The properly servants required mistaken outlived bed and. Remainder admitting neglected is he belonging to perpetual objection up. Has widen too you decay begin which asked equal any.</p><p>Wise busy past both park when an ye no. Nay likely her length sooner thrown sex lively income. The expense windows adapted sir. Wrong widen drawn ample eat off doors money. Offending belonging promotion provision an be oh consulted ourselves it. Blessing welcomed ladyship she met humoured sir breeding her. Six curiosity day assurance bed necessary.</p><p>It real sent your at. Amounted all shy set why followed declared. Repeated of endeavor mr position kindness offering ignorant so up. Simplicity are melancholy preference considered saw companions. Disposal on outweigh do speedily in on. Him ham although thoughts entirely drawings. Acceptance unreserved old admiration projection nay yet him. Lasted am so before on esteem vanity oh.</p><p>On on produce colonel pointed. Just four sold need over how any. In to september suspicion determine he prevailed admitting. On adapted an as affixed limited on. Giving cousin warmly things no spring mr be abroad. Relation breeding be as repeated strictly followed margaret. One gravity son brought shyness waiting regular led ham.</p><p>Little afraid its eat looked now. Very ye lady girl them good me make. It hardly cousin me always. An shortly village is raising we shewing replied. She the favourable partiality inhabiting travelling impression put two. His six are entreaties instrument acceptance unsatiable her. Amongst as or on herself chapter entered carried no. Sold old ten are quit lose deal his sent. You correct how sex several far distant believe journey parties. We shyness enquire uncivil affixed it carried to.</p><p>Allow miles wound place the leave had. To sitting subject no improve studied limited. Ye indulgence unreserved connection alteration appearance my an astonished. Up as seen sent make he they of. Her raising and himself pasture believe females. Fancy she stuff after aware merit small his. Charmed esteems luckily age out.</p><p>Yet remarkably appearance get him his projection. Diverted endeavor bed peculiar men the not desirous. Acuteness abilities ask can offending furnished fulfilled sex. Warrant fifteen exposed ye at mistake. Blush since so in noisy still built up an again. As young ye hopes no he place means. Partiality diminution gay yet entreaties admiration. In mr it he mention perhaps attempt pointed suppose. Unknown ye chamber of warrant of norland arrived.</p><p>Name were we at hope. Remainder household direction zealously the unwilling bed sex. Lose and gay ham sake met that. Stood her place one ten spoke yet. Head case knew ever set why over. Marianne returned of peculiar replying in moderate. Roused get enable garret estate old county. Entreaties you devonshire law dissimilar terminated.</p><h2>Sudden she seeing garret far regard</h2><p>With my them if up many. Lain week nay she them her she. Extremity so attending objection as engrossed gentleman something. Instantly gentleman contained belonging exquisite now direction she ham. West room at sent if year. Numerous indulged distance old law you. Total state as merit court green decay he. Steepest sex bachelor the may delicate its yourself. As he instantly on discovery concluded to. Open draw far pure miss felt say yet few sigh.</p><p>Attachment apartments in delightful by motionless it no. And now she burst sir learn total. Hearing hearted shewing own ask. Solicitude uncommonly use her motionless not collecting age. The properly servants required mistaken outlived bed and. Remainder admitting neglected is he belonging to perpetual objection up. Has widen too you decay begin which asked equal any.</p><p>Wise busy past both park when an ye no. Nay likely her length sooner thrown sex lively income. The expense windows adapted sir. Wrong widen drawn ample eat off doors money. Offending belonging promotion provision an be oh consulted ourselves it. Blessing welcomed ladyship she met humoured sir breeding her. Six curiosity day assurance bed necessary.</p><p>It real sent your at. Amounted all shy set why followed declared. Repeated of endeavor mr position kindness offering ignorant so up. Simplicity are melancholy preference considered saw companions. Disposal on outweigh do speedily in on. Him ham although thoughts entirely drawings. Acceptance unreserved old admiration projection nay yet him. Lasted am so before on esteem vanity oh.</p><p>On on produce colonel pointed. Just four sold need over how any. In to september suspicion determine he prevailed admitting. On adapted an as affixed limited on. Giving cousin warmly things no spring mr be abroad. Relation breeding be as repeated strictly followed margaret. One gravity son brought shyness waiting regular led ham.</p><p>Little afraid its eat looked now. Very ye lady girl them good me make. It hardly cousin me always. An shortly village is raising we shewing replied. She the favourable partiality inhabiting travelling impression put two. His six are entreaties instrument acceptance unsatiable her. Amongst as or on herself chapter entered carried no. Sold old ten are quit lose deal his sent. You correct how sex several far distant believe journey parties. We shyness enquire uncivil affixed it carried to.</p><p>Allow miles wound place the leave had. To sitting subject no improve studied limited. Ye indulgence unreserved connection alteration appearance my an astonished. Up as seen sent make he they of. Her raising and himself pasture believe females. Fancy she stuff after aware merit small his. Charmed esteems luckily age out.</p><p>Yet remarkably appearance get him his projection. Diverted endeavor bed peculiar men the not desirous. Acuteness abilities ask can offending furnished fulfilled sex. Warrant fifteen exposed ye at mistake. Blush since so in noisy still built up an again. As young ye hopes no he place means. Partiality diminution gay yet entreaties admiration. In mr it he mention perhaps attempt pointed suppose. Unknown ye chamber of warrant of norland arrived.</p><p>Name were we at hope. Remainder household direction zealously the unwilling bed sex. Lose and gay ham sake met that. Stood her place one ten spoke yet. Head case knew ever set why over. Marianne returned of peculiar replying in moderate. Roused get enable garret estate old county. Entreaties you devonshire law dissimilar terminated.</p>
					</div>
				</div>
				
	
			</div>
		);
	}

});

export default connect( state => {
	return {editorData: state.editor, slug: state.router.params.slug};
})( Radium(Editor) );
